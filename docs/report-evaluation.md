# Parameter Recovery Report Evaluation

This document explains the evaluation criteria for parameter recovery and validation reports in this project.

## Overview

The `/evaluate-report` command provides automated quality assessment of statistical validation reports, ensuring they meet rigorous standards before being used to validate modeling approaches on real data.

## Why Report Evaluation Matters

Parameter recovery validation is critical in statistical modeling:

1. **Prevents false confidence** - Poorly validated models can produce misleading results
2. **Catches methodological errors** - Common anti-patterns (like testing on training data) invalidate findings
3. **Ensures reproducibility** - Well-documented validation enables replication
4. **Guides model selection** - Proper validation reveals when simpler models suffice

## Evaluation Criteria

### 1. Coverage Validation (Critical)

**What it checks:**
- Confidence intervals contain true parameter values in ~95% of simulations
- Tests conducted on NEW datasets, not training data
- Differences between groups tested, not just individual parameters
- **Coverage checked at observation-level, not just global mean**

**Why it matters:**
- Fundamental test of statistical validity
- Miscalibrated CIs lead to wrong conclusions
- Testing on training data gives artificially perfect 100% coverage
- **Global-mean-only coverage is too lenient and hides CI miscalibration**

**Common issues:**
- ‚ùå Testing models on their own training data
- ‚ùå Only testing individual parameters instead of differences
- ‚ùå Coverage rates outside 93-97% range
- ‚ùå **Checking if mean prediction is covered, not individual observations**
- ‚ùå **Coverage rates suspiciously high (>98%) indicating overly conservative CIs**

**Best practices:**
- ‚úÖ Fit MANY models on MANY new datasets
- ‚úÖ Check if each CI contains the true value
- ‚úÖ Report coverage for all comparisons of interest
- ‚úÖ **Check coverage for EACH observation/timepoint individually**
- ‚úÖ **Aggregate observation-level coverage to get empirical coverage rate**

### 1.5. Parameter Recovery Validation (Critical for Dispersion/Variance Parameters)

**What it checks:**
- "True" parameter values match the data generation process
- Comparison metrics align with model family
- Theoretical formulas match empirical estimation methods

**Why it matters:**
- **Beta-binomial theoretical dispersion ‚â† quasi-binomial residual dispersion**
- Using wrong "true" values invalidates recovery assessment
- Mixing parameterizations produces misleading bias estimates

**Common issues specific to overdispersion:**
- ‚ùå **Using beta-binomial `œÜ_BB = (œÜ + n)/(œÜ + 1)` to validate quasi-binomial `œÜ_QB`**
  - These are different dispersion measures!
  - Beta-binomial: parameter of conjugate distribution
  - Quasi-binomial: residual variance inflation from Pearson œá¬≤
- ‚ùå **Comparing theoretical formulas to empirical estimates without validation**
- ‚ùå **Not checking empirical variance against theoretical predictions**

**Best practices for dispersion parameter recovery:**
- ‚úÖ **Use empirical overdispersion as "true" value:**
  ```r
  empirical_var <- var(successes / trials)
  binomial_var <- p * (1 - p) / n
  phi_true <- empirical_var / binomial_var
  ```
- ‚úÖ **OR use input œÜ directly if validating concentration parameter**
- ‚úÖ **Document which dispersion measure you're validating**
- ‚úÖ **Plot estimated vs true on same scale (not mixing parameterizations)**

### 2. DGP Consistency (Critical)

**What it checks:**
- All simulations use identical data-generating process (DGP)
- Parameters match between reference data and validation runs
- Default function parameters documented explicitly

**Why it matters:**
- Parameter mismatches cause systematic bias
- Inconsistent DGP invalidates coverage calculations
- Undocumented defaults lead to confusion and errors

**Project learnings:**
- When we used `trend_slope=0.0003` in reference data but default `trend_slope=0` in validation, coverage dropped from 94% to 73%
- Fixing this single parameter alignment restored proper coverage

**Best practices:**
- ‚úÖ Document ALL parameters with explicit values
- ‚úÖ Use named parameters consistently: `baseline_rates = c(phd = 0.040, ...)`
- ‚úÖ Add "What We're Comparing" section explaining DGP

### 3. Bias Quantification

**What it checks:**
- Mean estimation error reported (should be near zero)
- Precision (SD of estimates) quantified
- Bias distribution visualized
- Relative bias calculated (% of true value)

**Why it matters:**
- Small systematic bias acceptable; large bias indicates problems
- Precision shows estimation variability
- Together, bias and precision characterize estimation quality

**Best practices:**
- ‚úÖ Report bias, precision, RMSE for all parameters
- ‚úÖ Create bias distribution plots (should be symmetric around zero)
- ‚úÖ Compare bias to practical significance thresholds (<5% ideally)

### 4. False Positive Control

**What it checks:**
- Type I error rate tested with null simulations
- All groups have identical parameters (no true differences)
- False positive rate ‚â§5% (ideally 3-7%)

**Why it matters:**
- High false positive rate means spurious findings
- Must control Type I error to trust significant results
- Different tests (baseline, trend, seasonal) may have different rates

**Best practices:**
- ‚úÖ Run 200+ null simulations
- ‚úÖ Test: baseline differences, trend differences, and combined
- ‚úÖ Report rate with 95% CI (using binomial distribution)

### 5. Visualization Quality

**What it checks:**
- 3+ diagnostic plots included
- Component-wise visualization (trend, seasonal, baseline)
- Uncertainty visualization (error bars, ribbons, CI plots)
- Clear labels and legends

**Why it matters:**
- Visual inspection catches issues that summary statistics miss
- Diagnostic plots reveal patterns in bias, coverage, precision
- Good visualization aids interpretation and communication

**Best practices:**
- ‚úÖ Raw data + fitted values overlay
- ‚úÖ Coverage rate by parameter/comparison
- ‚úÖ Bias distribution (histogram or density)
- ‚úÖ CI width analysis (precision across time/space)
- ‚úÖ Effect size plots with confidence intervals

### 6. Report Structure

**Required sections:**
- **Overview** - What's being validated and why
- **Why This Matters** - Real-world implications
- **Model Specifications** - Exact formulas for all models
- **Coverage Results** - Tables and plots showing coverage rates
- **Bias Analysis** - Mean bias, SD, RMSE by parameter type
- **False Positive Test** - Null simulation results (if applicable)
- **Interpretation** - Clear Pass/Fail with actionable insights

**Why it matters:**
- Standardized structure ensures completeness
- Readers can quickly assess validation quality
- Missing sections indicate incomplete validation

### 7. Statistical Rigor

**What it checks:**
- Sufficient simulations (‚â•200 for coverage, ‚â•100 for model selection)
- Proper CI construction (accounting for covariance between parameters)
- Multiple comparisons awareness
- Realistic parameter values (based on real data patterns)
- Parallel processing for computational efficiency

**Why it matters:**
- Too few simulations give unreliable estimates of coverage/bias
- Ignoring covariance miscalibrates CIs for differences
- Unrealistic parameters make validation irrelevant to real analysis

**Best practices:**
- ‚úÖ Use 200-500 simulations for coverage validation
- ‚úÖ Extract covariance matrix: `vcov(model)`
- ‚úÖ Base simulation parameters on real CPS data ranges
- ‚úÖ Document computational settings (cores, runtime)

### 8. Documentation Quality

**What it checks:**
- All DGP parameters stated explicitly with values
- "What We're Comparing" section explaining test procedure
- Reproducible code (seeds, package versions)
- Computational notes (runtime, parallelization)

**Why it matters:**
- Enables exact replication by others
- Clarifies what validation actually tests
- Prevents confusion about defaults and assumptions

**Best practices:**
- ‚úÖ State ALL parameters: `baseline_rates = c(phd = 0.040, masters = 0.050, bachelors = 0.060)`
- ‚úÖ Explain step-by-step: "1. Simulate data with known parameters, 2. Fit model, 3. Check if CI contains true value"
- ‚úÖ Set seed: `set.seed(42)` for reproducibility
- ‚úÖ Note software versions: R 4.4.0, mgcv 1.9-1

## Anti-Patterns to Avoid

### 1. Training Data Reuse (Critical Error)

**The mistake:**
```r
# WRONG: Testing model on its own training data
sim_data <- simulate_data(...)
model <- fit_model(sim_data)
ci <- confint(model)  # CI will contain fitted value with ~100% probability!
```

**The fix:**
```r
# CORRECT: Test on NEW data
for (i in 1:n_sims) {
  sim_data <- simulate_data(seed = i)  # NEW dataset each time
  model <- fit_model(sim_data)
  ci <- confint(model)
  covered[i] <- (true_param >= ci[1]) & (true_param <= ci[2])
}
mean(covered)  # Should be ~0.95
```

**Why this matters:**
- Testing on training data guarantees ~100% coverage
- Gives false confidence in model validity
- One of the most common validation errors

### 2. DGP Mismatch

**The mistake:**
```r
# Reference data uses trend_slope = 0.0003
ref_data <- simulate_data(trend_slope = 0.0003)

# But validation uses default trend_slope = 0
validate_coverage()  # Uses default!
```

**The fix:**
```r
# Explicitly match ALL parameters
validate_coverage(trend_slope = 0.0003)  # Matches reference data
```

### 3. Prediction Interval Confusion

**The mistake:**
Using prediction interval methodology for parameter CIs:
```r
# WRONG for parameter validation
model <- fit_model(data)
predict(model, newdata = test_data)  # This is for predictions, not parameter validation!
```

**Correct approaches:**
- **For parameter CIs:** Fit many models, check if each CI contains true value
- **For prediction intervals:** Fit ONE model, test on many new datasets

### 4. Missing Covariance

**The mistake:**
```r
# WRONG: Ignoring covariance for differences
diff <- coef[1] - coef[2]
se_diff <- sqrt(se[1]^2 + se[2]^2)  # Missing covariance term!
```

**The fix:**
```r
# CORRECT: Include covariance
V <- vcov(model)
se_diff <- sqrt(V[1,1] + V[2,2] - 2*V[1,2])
```

### 5. Dispersion Parameter Mismatch (Quasi-Binomial Specific)

**The mistake:**
```r
# WRONG: Mixing beta-binomial and quasi-binomial dispersion measures
generate_overdispersed_binomial <- function(phi = 20, n_total = 1000, ...) {
  # ... beta-binomial data generation ...

  # This is beta-binomial theoretical dispersion!
  theoretical_dispersion <- (phi + n_total) / (phi + 1)  # ‚âà 1001/21 = 47.7

  return(list(data = data, phi_true = theoretical_dispersion))
}

# Later: Compare to quasi-binomial estimate
model <- gam(cbind(y, n-y) ~ ..., family = quasibinomial())
phi_est <- summary(model)$dispersion  # Pearson œá¬≤ / df ‚âà 45

# WRONG comparison: these are different dispersion measures!
bias <- phi_est - phi_true  # Comparing apples to oranges
```

**The fix (Option A - Use empirical dispersion):**
```r
# CORRECT: Calculate empirical overdispersion from generated data
generate_overdispersed_binomial <- function(phi = 20, n_total = 1000, true_p = 0.05, ...) {
  # Generate data
  alpha <- true_p * phi
  beta <- (1 - true_p) * phi
  true_probs <- rbeta(n_obs, alpha, beta)
  y <- rbinom(n_obs, size = n_total, prob = true_probs)

  # Calculate EMPIRICAL overdispersion (what quasi-binomial will estimate)
  p_hat <- y / n_total
  empirical_var <- var(p_hat)
  binomial_var <- true_p * (1 - true_p) / n_total
  phi_empirical <- empirical_var / binomial_var

  return(list(data = data, phi_true = phi_empirical))
}
```

**The fix (Option B - Use input œÜ directly):**
```r
# CORRECT: Just use the input concentration parameter
phi_results <- data.frame(
  phi_true = phi,  # Input concentration parameter
  phi_est = summary(model)$dispersion  # Quasi-binomial estimate
)
```

**Why this matters:**
- Beta-binomial `(œÜ + n)/(œÜ + 1)` inflates with sample size n
- Quasi-binomial dispersion is **scale-free** (variance inflation factor)
- Comparing them produces:
  - Misleading bias estimates
  - Wrong "true" values on recovery plots
  - Incorrect slope on identity line comparisons

### 6. Global-Mean Coverage Check (Too Lenient)

**The mistake:**
```r
# WRONG: Only checking if overall mean is covered
for (i in 1:n_sims) {
  sim_data <- simulate_data()
  model <- fit_model(sim_data)

  # Average predictions across all observations
  pred <- predict(model, type = "link", se.fit = TRUE)
  mean_pred <- mean(pred$fit)
  mean_se <- mean(pred$se.fit)

  ci_lower <- plogis(mean_pred - 1.96 * mean_se)
  ci_upper <- plogis(mean_pred + 1.96 * mean_se)

  # Only checks if GLOBAL MEAN is covered!
  covered[i] <- (ci_lower <= true_p_mean) && (ci_upper >= true_p_mean)
}

mean(covered)  # Will be ~100%, not ~95%!
```

**The fix:**
```r
# CORRECT: Check coverage for EACH observation
for (i in 1:n_sims) {
  sim_data <- simulate_data()
  model <- fit_model(sim_data)

  # Get predictions for each observation
  pred <- predict(model, type = "link", se.fit = TRUE)

  # Check coverage for EACH observation individually
  coverage_vector <- rep(FALSE, n_obs)
  for (j in 1:n_obs) {
    ci_lower <- plogis(pred$fit[j] - 1.96 * pred$se.fit[j])
    ci_upper <- plogis(pred$fit[j] + 1.96 * pred$se.fit[j])

    # Compare to TRUE value for this observation
    true_prob <- sim_data$true_prob[j]
    coverage_vector[j] <- (ci_lower <= true_prob) && (ci_upper >= true_prob)
  }

  # Coverage rate for this simulation
  coverage_rate[i] <- mean(coverage_vector)
}

# Should be ~0.95 across simulations
mean(coverage_rate)
```

**Why this matters:**
- Global-mean-only checks are too lenient
- Gives artificially high coverage (often 100%)
- Hides CI miscalibration at observation level
- Temporal variation is ignored (model includes time trends!)

### 7. Confidence Interval vs Prediction Interval Confusion (Critical)

**The mistake:**
```r
# WRONG: Using confidence intervals for predictions when testing coverage
for (i in 1:n_sims) {
  train_data <- simulate_data()
  model <- fit_model(train_data)

  # Generate NEW holdout observations
  holdout_data <- simulate_data(n = 100)
  pred <- predict(model, newdata = holdout_data, se.fit = TRUE)

  # Confidence interval (for MEAN prediction)
  ci_lower <- pred$fit - 1.96 * pred$se.fit
  ci_upper <- pred$fit + 1.96 * pred$se.fit

  # WRONG: Testing if individual observations fall in CI
  coverage[i] <- mean((ci_lower <= holdout_data$y) & (holdout_data$y <= ci_upper))
}

# Coverage will be ~30%, not ~95% (too narrow!)
```

**The fix:**
```r
# CORRECT: Use prediction intervals that include residual variance
for (i in 1:n_sims) {
  train_data <- simulate_data()
  model <- fit_model(train_data)

  # Generate NEW holdout observations
  holdout_data <- simulate_data(n = 100)
  pred <- predict(model, newdata = holdout_data, se.fit = TRUE)

  # Prediction interval (for NEW observations)
  # Includes: parameter uncertainty (se.fit) + residual variance (sigma)
  residual_var <- summary(model)$dispersion  # or sigma(model)^2 for Gaussian
  pred_se <- sqrt(pred$se.fit^2 + residual_var)

  pi_lower <- pred$fit - 1.96 * pred_se
  pi_upper <- pred$fit + 1.96 * pred_se

  # CORRECT: Now tests prediction intervals
  coverage[i] <- mean((pi_lower <= holdout_data$y) & (holdout_data$y <= pi_upper))
}

# Coverage should be ~95%
```

**The distinction**:

| Interval Type | Purpose | Formula | Width | Use Case |
|---------------|---------|---------|-------|----------|
| **Confidence Interval** | Uncertainty about mean | ŒºÃÇ ¬± t¬∑SE(ŒºÃÇ) | Narrow | Parameter estimation, mean prediction |
| **Prediction Interval** | Uncertainty about new observation | ≈∑ ¬± t¬∑‚àö(SE(≈∑)¬≤ + œÉ¬≤) | Wide | Forecasting, validation |

**Why this matters**:
- CIs only capture **parameter uncertainty**
- PIs capture **parameter uncertainty + residual variance**
- Testing new observations with CIs gives severe under-coverage
- This is one of the most common validation errors!

**Special cases**:

**For binomial/quasi-binomial models**:
```r
# On link scale (logit)
pred_link <- predict(model, newdata = holdout, type = "link", se.fit = TRUE)

# Confidence interval (for mean probability)
ci_link <- pred_link$fit + c(-1.96, 1.96) * pred_link$se.fit
ci_prob <- plogis(ci_link)

# For prediction interval, need to add residual variance
# But binomial residual variance is p(1-p), already in the data!
# So PI construction is more complex - often use simulation instead
```

**Recommendation for binomial data**:
1. Use simulation-based prediction intervals
2. Or test coverage of **smooth/mean function**, not individual observations
3. Document clearly what you're testing!

### 8. Quasi-Binomial "Parameter Recovery" Mistake (Critical)

**The fundamental mistake:**
Attempting to do parameter recovery validation for quasi-binomial models.

**Why this is wrong:**
```r
# WRONG: Treating quasi-binomial as a generative model
generate_data <- function() {
  # Generate from beta-binomial
  true_probs <- rbeta(n, alpha, beta)
  y <- rbinom(n, size = N, prob = true_probs)

  return(list(data = y, true_dispersion = ...))  # What is "true"?
}

model <- gam(y ~ ..., family = quasibinomial())
phi_est <- summary(model)$dispersion

# WRONG: Compare to what? Quasi-binomial has no generative model!
bias <- phi_est - true_dispersion
```

**Why quasi-binomial has no generative model:**
- Quasi-binomial is a **variance adjustment method**, not a probability distribution
- It inflates SEs by `‚àöœÜ` where œÜ = Pearson œá¬≤/df
- You cannot sample from a quasi-binomial distribution
- There is no "true dispersion parameter" to recover

**The correct approach - Robustness testing:**
```r
# CORRECT: Test robustness to overdispersion
# 1. Generate from realistic overdispersed DGP (e.g., beta-binomial)
generate_betabinomial_data <- function(phi = 20) {
  true_probs <- rbeta(n, alpha = p*phi, beta = (1-p)*phi)
  y <- rbinom(n, size = N, prob = true_probs)
  return(y)
}

# 2. Fit both binomial and quasi-binomial
model_binom <- gam(y ~ ..., family = binomial())
model_quasi <- gam(y ~ ..., family = quasibinomial())

# 3. Test which provides better-calibrated prediction intervals
# Expected: Quasi-binomial performs better but still under-covers
# Why: Beta-binomial has extra variance that PIs don't account for
```

**What to test instead:**
1. **Model comparison**: Binomial vs Quasi-binomial on overdispersed data
2. **PI coverage**: Which model has better-calibrated prediction intervals?
3. **Robustness**: How does quasi-binomial handle different overdispersion levels?
4. **Trade-offs**: Quasi-binomial = wider PIs + smoother fits (different REML Œª)

**Key findings from validation:**
- Quasi-binomial Œª ~64,000√ó larger than binomial (smoother fits)
- This is REML behavior (scale = œÜ), not just SE adjustment
- Quasi-binomial has better coverage than binomial on overdispersed data
- But neither achieves 95% coverage (DGP mismatch is expected)
- **Trade-off**: Wider PIs but smoother (potentially less wiggly) trends

**When to use quasi-binomial:**
- Real data shows overdispersion (œÜ > 2)
- Want more conservative (wider) prediction intervals
- Prefer smoother trends (less overfitting)
- Accept trade-off: less wiggly fits, more realistic uncertainty

**Document clearly:**
- This is NOT parameter recovery
- This tests robustness to model misspecification
- Beta-binomial DGP ‚â† quasi-binomial model (intentional mismatch)
- Under-coverage is expected and informative

## Using the Evaluation Tool

There are three ways to evaluate reports:

### 1. Interactive Mode (Recommended)

The easiest way - it automatically finds and lists available reports:

```bash
# Via npm
npm run report:evaluate

# Or directly
bash scripts/evaluate-report-interactive.sh
```

**What it does:**
- Lists all reports in `reports/` (HTML, QMD, Rmd files)
- Shows file size and modification date
- Highlights the most recently modified report
- Lets you select which one to evaluate (or press Enter for most recent)

**Example output:**
```
üìä Searching for reports in reports/ directory...

Found 12 report(s):

  1) factor-smooth-parameter-recovery.qmd [Quarto Source]
     Size: 36K | Modified: 2025-11-13
     Path: reports/factor-smooth-parameter-recovery.qmd

  2) factor-smooth-parameter-recovery.html [Rendered HTML]
     Size: 296K | Modified: 2025-11-13
     Path: reports/factor-smooth-parameter-recovery.html

Most recent: factor-smooth-parameter-recovery.qmd

Select report to evaluate (1-12) or press Enter for most recent:
```

### 2. Direct File Specification

If you know which report you want to evaluate:

```bash
# Via npm
npm run report:evaluate reports/factor-smooth-parameter-recovery.html

# Via bash wrapper
bash scripts/evaluate-report-interactive.sh reports/factor-smooth-parameter-recovery.html

# Direct R script
Rscript scripts/evaluate-report.R reports/factor-smooth-parameter-recovery.html
```

### 3. Slash Command

When using Claude Code:

```bash
/evaluate-report
```

Claude will:
1. List available reports
2. Ask which one to evaluate
3. Run the evaluation
4. Interpret results with recommendations

### Verbose Mode

For detailed diagnostics, add `--verbose`:

```bash
# Interactive
npm run report:evaluate:verbose

# Direct
bash scripts/evaluate-report-interactive.sh reports/your-report.html --verbose
```

## Interpreting Results

### Grades

- **PASS (‚â•80%)** - Report meets standards, validation is trustworthy
- **CONDITIONAL PASS (65-79%)** - Minor issues, address warnings before publication
- **FAIL (<65%)** - Critical issues, do not trust validation results

### Output Sections

1. **Overall Grade** - Summary pass/fail with percentage
2. **Category Scores** - Breakdown by evaluation criterion
3. **Critical Issues** - Must fix before trusting results
4. **Warnings** - Should address for robustness
5. **Strengths** - What the report does well
6. **Recommendations** - Specific actionable improvements

### Example Output

```
========================================
  Parameter Recovery Report Evaluation
========================================

Report: factor-smooth-parameter-recovery.html

===============================
  EVALUATION RESULTS
===============================

Overall Grade: PASS (95%)

Category Scores:
----------------
  Coverage:              100%  [Pass]
  Dgp_consistency:        80%  [Pass]
  Bias:                  100%  [Pass]
  False_positive:        100%  [Pass]
  Visualization:         100%  [Pass]
  Structure:             100%  [Pass]
  Rigor:                  80%  [Pass]
  Documentation:         100%  [Pass]

Warnings (Should Address):
--------------------------
  ‚ö†Ô∏è  No discussion of DGP consistency across simulations

Strengths:
----------
  ‚úÖ Strong coverage validation methodology
  ‚úÖ Excellent visualization and diagnostics
  ‚úÖ Well-documented and reproducible

Recommendations:
----------------
  ‚Ä¢ Add explicit discussion of DGP parameter consistency
  ‚Ä¢ Document default parameter values in function calls

========================================
```

## Integration with Project Workflow

### When to Evaluate

1. **After creating a new validation report** - Ensure quality before using results
2. **Before submitting for review** - Catch issues early
3. **When updating validation functions** - Verify reports still meet standards
4. **During code review** - Systematic quality check

### Continuous Improvement

The evaluation criteria are based on project learnings stored in `.claude/learnings/`:

- Coverage validation methodology insights
- DGP consistency lessons learned
- Anti-patterns discovered during development
- Best practices from statistical literature

As the project evolves, update both:
1. Evaluation criteria in `scripts/evaluate-report.R`
2. This documentation in `docs/report-evaluation.md`

## References

### Project Resources

- [`.claude/commands/evaluate-report.md`](../.claude/commands/evaluate-report.md) - Command documentation
- [`scripts/evaluate-report.R`](../scripts/evaluate-report.R) - Evaluation script
- [`R/parameter-recovery-validation.R`](../R/parameter-recovery-validation.R) - Validation functions
- [`.claude/learnings/`](../.claude/learnings/) - Project learnings archive

### Statistical Background

Key concepts in parameter recovery validation:

- **Coverage probability** - P(CI contains true parameter)
- **Bias** - E[estimate - true value]
- **Precision** - SD(estimates)
- **RMSE** - sqrt(bias¬≤ + variance)
- **Type I error** - P(reject H0 | H0 true)

For more on validation methodology, see:
- Gelman & Hill (2007) - Data Analysis Using Regression and Multilevel/Hierarchical Models
- McElreath (2020) - Statistical Rethinking, Chapter 4 (model validation)
- Wood (2017) - Generalized Additive Models: An Introduction with R, Chapter 5 (inference)

## Contributing

To improve the evaluation criteria:

1. Add learnings: `/learn <insight>`
2. Update evaluation script: `scripts/evaluate-report.R`
3. Update documentation: `docs/report-evaluation.md`
4. Add tests: `test/evaluate-report.test.js`
5. Run checks: `npm run hygiene`

Questions? Open an [issue](https://github.com/jflournoy/phd-unemployment-model/issues).
