# Targets Pipeline Documentation

This project uses the `targets` package for reproducible data analysis with intelligent caching and dependency tracking.

## Overview

The pipeline tracks dependencies from raw IPUMS data → processed datasets → models → reports. When you change code or data, `targets` automatically determines what needs to be re-run.

## Key Benefits

1. **Intelligent Caching**: Expensive computations (especially Bayesian models) run once until inputs change
2. **Dependency Tracking**: Change a function → only affected targets rebuild
3. **Parallel Execution**: Independent targets run simultaneously
4. **Reproducibility**: Clear DAG of all analysis steps

## Quick Start

```r
# Install targets if needed
install.packages("targets")
install.packages("tarchetypes")

# View the pipeline
targets::tar_visnetwork()

# Run the entire pipeline
targets::tar_make()

# Check what's outdated
targets::tar_outdated()

# Load a specific target into your session
targets::tar_load(education_counts)
targets::tar_read(model_binomial)  # Read without loading
```

## Pipeline Structure

### Data Processing Targets

- `raw_data`: IPUMS CPS microdata (from `data-raw/ipums_data.rds`)
- `education_counts`: Unweighted person counts by education (for binomial GAMs)
- `phd_monthly`: PhD unemployment time series
- `multi_education`: Weighted unemployment by all education levels

### Model Targets

- `model_binomial`: Binomial GAM with factor smooth
- `model_quasibinomial`: Quasi-binomial GAM (handles overdispersion)
- `model_comparison`: Comparison of binomial vs quasi-binomial

### Report Targets

- `report_binomial_quasi`: Quarto report comparing model families

## Common Workflows

### Update Data and Rebuild Everything

```r
# After updating IPUMS data
targets::tar_invalidate(raw_data)
targets::tar_make()
```

### Modify a Model and Re-fit

```r
# Edit model specification in _targets.R
targets::tar_make(model_quasibinomial)  # Only this model re-runs
```

### Add a New Model Variant

```r
# Add to _targets.R:
tar_target(
  model_beta_binomial,
  fit_beta_binomial_gam(education_counts)
)

# Run just the new target
targets::tar_make(model_beta_binomial)
```

### Parallel Execution

```r
# Use multiple cores for independent targets
targets::tar_make_clustermq(workers = 4)

# Or with future backend
targets::tar_make_future(workers = 4)
```

## File Organization

```
phd-unemployment-model/
├── _targets.R              # Pipeline definition
├── _targets/               # Cache directory (gitignored)
│   ├── meta/               # Target metadata
│   └── objects/            # Cached target outputs
├── data-raw/
│   ├── ipums_data.rds      # Raw IPUMS data (input)
│   └── generate-derived-data.R  # Standalone script (pre-targets)
├── data/                   # Derived datasets (generated by targets)
│   ├── education-spectrum-counts.rds
│   ├── phd-monthly-unemployment.rds
│   └── multi-education-unemployment.rds
└── reports/                # Quarto reports (managed by tar_quarto())
```

## Bayesian Models (Future)

When adding Stan/brms models, the caching becomes critical:

```r
# Example future target
tar_target(
  model_beta_binomial_stan,
  {
    brms::brm(
      n_unemployed | trials(n_total) ~
        s(time_index) + s(month, bs = "cc") +
        s(time_index, by = education),
      data = education_counts,
      family = beta_binomial(),
      chains = 4,
      cores = 4,
      iter = 2000
    )
  },
  # This might take 20-30 minutes - cache it!
  deployment = "main"
)
```

The first time this runs, it takes 30 minutes. But after that, it only re-runs if:
- The model formula changes
- The data changes
- The priors change
- The code in the target changes

## Inspecting the Pipeline

```r
# Visualize dependencies
targets::tar_visnetwork()

# See target status
targets::tar_progress()

# Get metadata about a target
targets::tar_meta(model_binomial)

# View target definition
targets::tar_manifest()
```

## Best Practices

1. **Keep targets focused**: Each target should do one thing
2. **Use `format = "file"` for file outputs**: Ensures proper tracking
3. **Document expected runtimes**: Especially for slow targets
4. **Use branches for parameter sweeps**: `tar_map()` for multiple parameter combinations
5. **Test targets individually**: `tar_make(target_name)` before full pipeline

## Debugging

```r
# Load all targets up to a specific one
targets::tar_load_globals()
targets::tar_load(education_counts)

# Then manually run the target code
# (copy from _targets.R)

# View errors
targets::tar_meta() %>%
  dplyr::filter(!is.na(error))

# Delete a problematic target and re-run
targets::tar_delete(model_binomial)
targets::tar_make(model_binomial)
```

## Standalone vs Targets Execution

The `data-raw/generate-derived-data.R` script can run standalone:

```bash
Rscript data-raw/generate-derived-data.R
```

This is useful for:
- Quick one-off data regeneration
- CI/CD pipelines
- When you don't need dependency tracking

But for iterative development with models, use `targets::tar_make()`.

## Further Reading

- [targets manual](https://books.ropensci.org/targets/)
- [tarchetypes documentation](https://docs.ropensci.org/tarchetypes/)
- [Debugging targets](https://books.ropensci.org/targets/debugging.html)
